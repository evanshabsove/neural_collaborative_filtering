{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Firstly, I know this paper is not new, and does not meet the qualification of 2019 or newer, but I feel as if this paper is so fundamental in Nurual collaborative filtering I felt that it would be good to take a look at it.\n",
    "\n",
    "### Brief overview\n",
    "\n",
    "Nural collaborative filtering is a way to recommend content to users based on their past interactions, and what other similar users have interacted with. It combines the results of Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) to learn and make predictions.\n",
    "\n",
    "### Changes made\n",
    "\n",
    "With these models being older, the code did not run on Pythin 3.10.16 in a andacond3 environment. I made changes to each of the model in order to make them runable.\n",
    "\n",
    "### Scientific contribution/messing around\n",
    "\n",
    "Both datasets used in the report (MovieLens and Pinterest) are large datasets which go off of a very specific metric. MovieLens has user ratings to key off of, and pinterest has list of users pins. I want to test the model on a seperate dataset which and see how well it performs, and if I can see the same results of increasing Hit Ratio (HR) from our Nural Collaborative Filter VS GMF or MLP.\n",
    "\n",
    "I have chosen board game reviews. The entire dataset can be found here\n",
    "\n",
    "https://www.kaggle.com/code/simonebergmann/collaborative-filter-for-boardgames\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: []\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18942215 entries, 0 to 18942214\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype  \n",
      "---  ------    -----  \n",
      " 0   BGGId     int64  \n",
      " 1   Rating    float64\n",
      " 2   Username  object \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 433.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "\n",
    "df_ratings = pd.read_csv(\"Data/board_games_data/user_ratings.csv\")\n",
    "print(df_ratings.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset size: 18942 rows\n",
      "           BGGId  Rating  UserId\n",
      "16597337   17709     2.0       0\n",
      "15421575  155693     2.0       1\n",
      "16051321   40531     7.0       2\n",
      "16392352   22864     7.0       3\n",
      "5484430       50     8.0       4\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18942 entries, 16597337 to 15579506\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   BGGId   18942 non-null  int64  \n",
      " 1   Rating  18942 non-null  float64\n",
      " 2   UserId  18942 non-null  int64  \n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 591.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Given the size of the databse lets take a smaller sample to save our computer\n",
    "df_ratings = df_ratings.sample(frac=0.001, random_state=42)\n",
    "print(f\"Sampled dataset size: {len(df_ratings)} rows\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Map users to integers\n",
    "user_mapping = {user: idx for idx, user in enumerate(df_ratings['Username'].unique())}\n",
    "df_ratings['UserId'] = df_ratings['Username'].map(user_mapping)\n",
    "df_ratings = df_ratings.drop(columns=['Username'])\n",
    "\n",
    "print(df_ratings.head())\n",
    "print(df_ratings.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['user_input', 'item_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7985 - loss: 0.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['user_input', 'item_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR: 0.020849828450778568, NDCG: 0.010082173126259572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_ratings = df_ratings.groupby(['UserId', 'BGGId'], as_index=False).mean()\n",
    "\n",
    "train, test = train_test_split(df_ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reindex UserId and BGGId in train and test to ensure zero-indexed and sequential values\n",
    "user_mapping = {user: idx for idx, user in enumerate(pd.concat([train['UserId'], test['UserId']]).unique())}\n",
    "item_mapping = {item: idx for idx, item in enumerate(pd.concat([train['BGGId'], test['BGGId']]).unique())}\n",
    "\n",
    "train['UserId'] = train['UserId'].map(user_mapping)\n",
    "train['BGGId'] = train['BGGId'].map(item_mapping)\n",
    "test['UserId'] = test['UserId'].map(user_mapping)\n",
    "test['BGGId'] = test['BGGId'].map(item_mapping)\n",
    "\n",
    "# Import GMF Model\n",
    "from GMF import get_model, parse_args\n",
    "from GMF import get_train_instances\n",
    "from GMF import evaluate_model\n",
    "from scipy.sparse import dok_matrix\n",
    "from CustomDataset import CustomDataset\n",
    "\n",
    "# Get the number of unique users and items\n",
    "num_users = len(user_mapping)\n",
    "num_items = len(item_mapping)\n",
    "\n",
    "# Define model parameters\n",
    "mf_dim = 8  # Embedding size for MF\n",
    "reg_mf = [0, 0]  # Regularization for MF embeddings\n",
    "\n",
    "# Initialize GMF model\n",
    "gmf_model = get_model(num_users, num_items, mf_dim, reg_mf)\n",
    "gmf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Vectorize the training data\n",
    "train['UserId'] = pd.factorize(train['UserId'])[0]\n",
    "train['BGGId'] = pd.factorize(train['BGGId'])[0]\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(train, test, num_users, num_items)\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "\n",
    "# Generate training instances\n",
    "num_negatives = 4  # Number of negative samples\n",
    "user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "\n",
    "# Train the model\n",
    "gmf_model.fit([np.array(user_input), np.array(item_input)],  # Inputs\n",
    "          np.array(labels),  # Labels\n",
    "          batch_size=256,\n",
    "          epochs=1,\n",
    "          verbose=1)\n",
    "\n",
    "# Prepare test data\n",
    "test_ratings = test[['UserId', 'BGGId', 'Rating']].values\n",
    "# Generate negative samples for testing\n",
    "test_negatives = []\n",
    "all_items = set(range(num_items))  # All possible item IDs\n",
    "for _, row in test.iterrows():\n",
    "    user = row['UserId']\n",
    "    positive_item = row['BGGId']\n",
    "    # Exclude items the user interacted with in the training set\n",
    "    interacted_items = {i for _, i in train.keys() if _ == user}\n",
    "    negative_items = list(all_items - interacted_items)\n",
    "    negative_samples = np.random.choice(negative_items, size=99, replace=False).tolist()  # 99 negatives\n",
    "    test_negatives.append(negative_samples)\n",
    "\n",
    "# Evaluate the model\n",
    "hits, ndcgs = evaluate_model(gmf_model, test_ratings, test_negatives, 10, 1)\n",
    "print(f\"HR: {np.mean(hits)}, NDCG: {np.mean(ndcgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7719 - loss: 0.5804\n",
      "HR: 0.46582211665347056, NDCG: 0.27873473940724885\n"
     ]
    }
   ],
   "source": [
    "# Run MLP\n",
    "from MLP import get_model, parse_args\n",
    "from MLP import get_train_instances\n",
    "from MLP import evaluate_model\n",
    "\n",
    "# Define model parameters\n",
    "layers = [64,32,16,8]  # Embedding size for MF\n",
    "reg_layers = [0,0,0,0]  # Regularization for MF embeddings\n",
    "\n",
    "# Initialize MLP model\n",
    "# def get_model(num_users, num_items, layers=[20, 10], reg_layers=[0, 0]):\n",
    "mlp_model = get_model(num_users, num_items, layers, reg_layers)\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the MLP model\n",
    "mlp_model.fit([np.array(user_input), np.array(item_input)],  # Inputs\n",
    "          np.array(labels),  # Labels\n",
    "          batch_size=256,\n",
    "          epochs=1,\n",
    "          verbose=1)\n",
    "# Evaluate the MLP model\n",
    "hits, ndcgs = evaluate_model(mlp_model, test_ratings, test_negatives, 10, 1)\n",
    "print(f\"HR: {np.mean(hits)}, NDCG: {np.mean(ndcgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7982 - loss: 0.5741\n",
      "HR: 0.4745315386645553, NDCG: 0.2857013866550117\n"
     ]
    }
   ],
   "source": [
    "# Run NeuMF\n",
    "from NeuMF import get_model, parse_args\n",
    "from NeuMF import get_train_instances\n",
    "from NeuMF import evaluate_model\n",
    "\n",
    "# Define model parameters\n",
    "layers = [64,32,16,8]\n",
    "reg_layers = [0,0,0,0]\n",
    "nm_factors = 8\n",
    "mf_dim = 8\n",
    "reg_mf = 0\n",
    "\n",
    "# def get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "# Initialize NeuMF model\n",
    "neumf_model = get_model(num_users, num_items, mf_dim, layers, reg_layers, reg_mf)\n",
    "neumf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Train the NeuMF model\n",
    "neumf_model.fit([np.array(user_input), np.array(item_input)],  # Inputs\n",
    "          np.array(labels),  # Labels\n",
    "          batch_size=256,\n",
    "          epochs=1,\n",
    "          verbose=1)\n",
    "# Evaluate the NeuMF model\n",
    "hits, ndcgs = evaluate_model(neumf_model, test_ratings, test_negatives, 10, 1)\n",
    "print(f\"HR: {np.mean(hits)}, NDCG: {np.mean(ndcgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From using a different dataset we see the same results that the neural collaborative filtering in provides a better HR % than MLF or GMF. This gives us a very powerful tool that we can use for content recomendation engines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
